{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction\n",
    "import io\n",
    "import pandas as pd\n",
    "import requests\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@data_loader\n",
    "def load_data_from_api(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template for loading data from API\n",
    "    \"\"\"\n",
    "    url = 'https://storage.googleapis.com/uber-1215/uber_data.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return pd.read_csv(io.StringIO(response.text), sep=',')\n",
    "\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforming\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from mage_ai.settings.repo import get_repo_path\n",
    "if 'transformer' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import transformer\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "\n",
    "@transformer\n",
    "def transform(df, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template code for a transformer block.\n",
    "\n",
    "    Add more parameters to this function if this block has multiple parent blocks.\n",
    "    There should be one parameter for each output variable from each parent block.\n",
    "\n",
    "    Args:\n",
    "        data: The output from the upstream parent block\n",
    "        args: The output from any additional upstream blocks (if applicable)\n",
    "\n",
    "    Returns:\n",
    "        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n",
    "    \"\"\"\n",
    "    # Specify your transformation logic here\n",
    "    output_dir = os.path.join(get_repo_path(), 'output')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "    datetime_dim = df[['tpep_pickup_datetime','tpep_dropoff_datetime']].drop_duplicates().reset_index(drop=True)\n",
    "    datetime_dim['pick_hour'] = datetime_dim['tpep_pickup_datetime'].dt.hour\n",
    "    datetime_dim['pick_day'] = datetime_dim['tpep_pickup_datetime'].dt.day\n",
    "    datetime_dim['pick_month'] = datetime_dim['tpep_pickup_datetime'].dt.month\n",
    "    datetime_dim['pick_year'] = datetime_dim['tpep_pickup_datetime'].dt.year\n",
    "    datetime_dim['pick_weekday'] = datetime_dim['tpep_pickup_datetime'].dt.weekday\n",
    "\n",
    "    datetime_dim['drop_hour'] = datetime_dim['tpep_dropoff_datetime'].dt.hour\n",
    "    datetime_dim['drop_day'] = datetime_dim['tpep_dropoff_datetime'].dt.day\n",
    "    datetime_dim['drop_month'] = datetime_dim['tpep_dropoff_datetime'].dt.month\n",
    "    datetime_dim['drop_year'] = datetime_dim['tpep_dropoff_datetime'].dt.year\n",
    "    datetime_dim['drop_weekday'] = datetime_dim['tpep_dropoff_datetime'].dt.weekday\n",
    "\n",
    "    datetime_dim['datetime_id'] = datetime_dim.index\n",
    "    datetime_dim = datetime_dim[['datetime_id', 'tpep_pickup_datetime', 'pick_hour', 'pick_day', 'pick_month', 'pick_year', 'pick_weekday',\n",
    "                             'tpep_dropoff_datetime', 'drop_hour', 'drop_day', 'drop_month', 'drop_year', 'drop_weekday']]\n",
    "\n",
    "    passenger_count_dim = df[['passenger_count']].drop_duplicates().reset_index(drop=True)\n",
    "    passenger_count_dim['passenger_count_id'] = passenger_count_dim.index\n",
    "    passenger_count_dim = passenger_count_dim[['passenger_count_id','passenger_count']]\n",
    "\n",
    "    trip_distance_dim = df[['trip_distance']].drop_duplicates().reset_index(drop=True)\n",
    "    trip_distance_dim['trip_distance_id'] = trip_distance_dim.index\n",
    "    trip_distance_dim = trip_distance_dim[['trip_distance_id','trip_distance']]\n",
    "    rate_code_type = {\n",
    "        1:\"Standard rate\",\n",
    "        2:\"JFK\",\n",
    "        3:\"Newark\",\n",
    "        4:\"Nassau or Westchester\",\n",
    "        5:\"Negotiated fare\",\n",
    "        6:\"Group ride\"\n",
    "    }\n",
    "\n",
    "    rate_code_dim = df[['RatecodeID']].drop_duplicates().reset_index(drop=True)\n",
    "    rate_code_dim['rate_code_id'] = rate_code_dim.index\n",
    "    rate_code_dim['rate_code_name'] = rate_code_dim['RatecodeID'].map(rate_code_type)\n",
    "    rate_code_dim = rate_code_dim[['rate_code_id','RatecodeID','rate_code_name']]\n",
    "\n",
    "\n",
    "    pickup_location_dim = df[['pickup_longitude', 'pickup_latitude']].drop_duplicates().reset_index(drop=True)\n",
    "    pickup_location_dim['pickup_location_id'] = pickup_location_dim.index\n",
    "    pickup_location_dim = pickup_location_dim[['pickup_location_id','pickup_latitude','pickup_longitude']] \n",
    "\n",
    "\n",
    "    dropoff_location_dim = df[['dropoff_longitude', 'dropoff_latitude']].drop_duplicates().reset_index(drop=True)\n",
    "    dropoff_location_dim['dropoff_location_id'] = dropoff_location_dim.index\n",
    "    dropoff_location_dim = dropoff_location_dim[['dropoff_location_id','dropoff_latitude','dropoff_longitude']]\n",
    "\n",
    "    payment_type_name = {\n",
    "        1:\"Credit card\",\n",
    "        2:\"Cash\",\n",
    "        3:\"No charge\",\n",
    "        4:\"Dispute\",\n",
    "        5:\"Unknown\",\n",
    "        6:\"Voided trip\"\n",
    "    }\n",
    "    payment_type_dim = df[['payment_type']].drop_duplicates().reset_index(drop=True)\n",
    "    payment_type_dim['payment_type_id'] = payment_type_dim.index\n",
    "    payment_type_dim['payment_type_name'] = payment_type_dim['payment_type'].map(payment_type_name)\n",
    "    payment_type_dim = payment_type_dim[['payment_type_id','payment_type','payment_type_name']]\n",
    "\n",
    "    fact_table = df.merge(passenger_count_dim, on='passenger_count') \\\n",
    "             .merge(trip_distance_dim, on='trip_distance') \\\n",
    "             .merge(rate_code_dim, on='RatecodeID') \\\n",
    "             .merge(pickup_location_dim, on=['pickup_longitude', 'pickup_latitude']) \\\n",
    "             .merge(dropoff_location_dim, on=['dropoff_longitude', 'dropoff_latitude'])\\\n",
    "             .merge(datetime_dim, on=['tpep_pickup_datetime','tpep_dropoff_datetime']) \\\n",
    "             .merge(payment_type_dim, on='payment_type') \\\n",
    "             [['VendorID', 'datetime_id', 'passenger_count_id',\n",
    "               'trip_distance_id', 'rate_code_id', 'store_and_fwd_flag', 'pickup_location_id', 'dropoff_location_id',\n",
    "               'payment_type_id', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "               'improvement_surcharge', 'total_amount']]\n",
    "    datetime_dim.to_csv(os.path.join(output_dir, 'datetime_dim.csv'), index=False)\n",
    "    passenger_count_dim.to_csv(os.path.join(output_dir, 'passenger_count_dim.csv'), index=False)\n",
    "    trip_distance_dim.to_csv(os.path.join(output_dir, 'trip_distance_dim.csv'), index=False)\n",
    "    rate_code_dim.to_csv(os.path.join(output_dir, 'rate_code_dim.csv'), index=False)\n",
    "    pickup_location_dim.to_csv(os.path.join(output_dir, 'pickup_location_dim.csv'), index=False)\n",
    "    dropoff_location_dim.to_csv(os.path.join(output_dir, 'dropoff_location_dim.csv'), index=False)\n",
    "    payment_type_dim.to_csv(os.path.join(output_dir, 'payment_type_dim.csv'), index=False)\n",
    "    fact_table.to_csv(os.path.join(output_dir, 'fact_table.csv'), index=False)\n",
    "\n",
    "    print(f\"Files saved in directory: {output_dir}\")\n",
    "    return {\"output_dir\": output_dir}\n",
    "\n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading\n",
    "import os\n",
    "import pandas as pd\n",
    "from mage_ai.settings.repo import get_repo_path\n",
    "from mage_ai.io.bigquery import BigQuery\n",
    "from mage_ai.io.config import ConfigFileLoader\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "\n",
    "@data_exporter\n",
    "def export_data_to_big_query(data, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Export multiple CSV files to Google Cloud BigQuery tables.\n",
    "    Specify your configuration settings in 'io_config.yaml'.\n",
    "\n",
    "    Docs: https://docs.mage.ai/design/data-loading#bigquery\n",
    "    \"\"\"\n",
    "    config_path = '/home/eugenecharleyacx87/uber-1215/io_config.yaml'\n",
    "    config_profile = 'default'\n",
    "    bigquery = BigQuery.with_config(ConfigFileLoader(config_path, config_profile))\n",
    "    \n",
    "    output_dir = os.path.join(get_repo_path(), 'output')\n",
    "    print(f\"Uploading files from directory: {output_dir}\")\n",
    "\n",
    "    table_mappings = {\n",
    "        'datetime_dim.csv': 'uber_data_engineering.datetime_dim',\n",
    "        'passenger_count_dim.csv': 'uber_data_engineering.passenger_count_dim',\n",
    "        'trip_distance_dim.csv': 'uber_data_engineering.trip_distance_dim',\n",
    "        'rate_code_dim.csv': 'uber_data_engineering.rate_code_dim',\n",
    "        'pickup_location_dim.csv': 'uber_data_engineering.pickup_location_dim',\n",
    "        'dropoff_location_dim.csv': 'uber_data_engineering.dropoff_location_dim',\n",
    "        'payment_type_dim.csv': 'uber_data_engineering.payment_type_dim',\n",
    "        'fact_table.csv': 'uber_data_engineering.fact_table'\n",
    "    }\n",
    "\n",
    "    for file_name, table_id in table_mappings.items():\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Uploading {file_name} to BigQuery table {table_id}...\")\n",
    "\n",
    "        bigquery.export(\n",
    "            df,\n",
    "            table_id,\n",
    "            if_exists='replace',  \n",
    "        )\n",
    "        print(f\"Successfully uploaded {file_name} to {table_id}\")\n",
    "\n",
    "    print(\"All files have been uploaded to BigQuery.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
